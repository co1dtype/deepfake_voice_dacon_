{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1hi1dibkHyFbaxAteLlZJw6r3g9ddd4Lf\n",
      "From (redirected): https://drive.google.com/uc?id=1hi1dibkHyFbaxAteLlZJw6r3g9ddd4Lf&confirm=t&uuid=19f496fc-41e8-4478-b6a1-2508f41c8bed\n",
      "To: /home/hyj/ChanHyung/Audio/DACON_fake_voice_detection/dataset/dacon_dataset.zip\n",
      "100%|██████████| 3.31G/3.31G [02:07<00:00, 26.0MB/s]\n",
      "Extracting files: 100%|██████████| 106708/106708 [00:24<00:00, 4304.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and extracted to ./dataset/dacon_dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_id = \"1hi1dibkHyFbaxAteLlZJw6r3g9ddd4Lf\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "download_path = \"./dataset/dacon_dataset.zip\"\n",
    "extract_path = \"./dataset/dacon_dataset\"\n",
    "\n",
    "if not os.path.exists(extract_path):\n",
    "    os.makedirs(extract_path)\n",
    "\n",
    "gdown.download(url, download_path, quiet=False)\n",
    "\n",
    "with ZipFile(download_path, 'r') as zip_ref:\n",
    "    for file in tqdm(zip_ref.namelist(), desc='Extracting files'):\n",
    "        zip_ref.extract(file, extract_path)\n",
    "\n",
    "os.remove(download_path)\n",
    "\n",
    "print(f\"File downloaded and extracted to {extract_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyj/anaconda3/envs/DACON_DEEPFAKE/lib/python3.11/site-packages/df/io.py:9: UserWarning: `torchaudio.backend.common.AudioMetaData` has been moved to `torchaudio.AudioMetaData`. Please update the import path.\n",
      "  from torchaudio.backend.common import AudioMetaData\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "\n",
    "from df import config\n",
    "from df.enhance import enhance, init_df, load_audio, save_audio\n",
    "from df.io import resample\n",
    "\n",
    "from torchaudio import AudioMetaData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hyj/ChanHyung/Audio/DACON_fake_voice_detection'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_path = os.getcwd()\n",
    "train_df = pd.read_csv(f'{base_path}/dataset/dacon_dataset/train.csv')\n",
    "test_df = pd.read_csv(f'{base_path}/dataset/dacon_dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 04:15:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on torch 2.3.0\u001b[0m\n",
      "\u001b[32m2024-07-23 04:15:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on host cvlab\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 04:15:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mLoading model settings of DeepFilterNet2\u001b[0m\n",
      "\u001b[32m2024-07-23 04:15:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mInitializing model `deepfilternet2`\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any of the parent directories): .git\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 04:15:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mFound checkpoint /home/hyj/ChanHyung/Audio/DACON_fake_voice_detection/DeepFilterNet2/checkpoints/model_96.ckpt.best with epoch 96\u001b[0m\n",
      "\u001b[32m2024-07-23 04:15:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on device cuda:0\u001b[0m\n",
      "\u001b[32m2024-07-23 04:15:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mModel loaded\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f520025427a4a40b1f15f31ef088b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 04:15:47\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[33m\u001b[1mAudio sampling rate does not match model sampling rate (32000, 48000). Resampling...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import shutil\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "dir_name = 'unlabeled_data_denoising'\n",
    "os.makedirs(f\"./dataset/dacon_dataset/{dir_name}\", exist_ok=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, df, _ = init_df(os.path.join(base_path, 'DeepFilterNet2'), config_allow_defaults=True)\n",
    "model = model.to(device=device).eval()\n",
    "test_x = os.listdir(os.path.join(base_path, 'dataset', 'dacon_dataset', 'unlabeled_data'))\n",
    "\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    id = test_x[i].split('/')[-1].split('.')[0]\n",
    "    try: \n",
    "        path = os.path.join(base_path, 'dataset', 'dacon_dataset', 'unlabeled_data', test_x[i])\n",
    "        sr = config(\"sr\", 32000, int, section=\"df\")\n",
    "        sample, meta = load_audio(path, sr)\n",
    "        enhanced = enhance(model, df, sample)\n",
    "\n",
    "        lim = torch.linspace(0.0, 1.0, int(sr * 0.15)).unsqueeze(0)\n",
    "        lim = torch.cat((lim, torch.ones(1, enhanced.shape[1] - lim.shape[1])), dim=1)\n",
    "        enhanced = enhanced * lim\n",
    "        if meta.sample_rate != sr:\n",
    "            enhanced = resample(enhanced, sr, meta.sample_rate)\n",
    "            sample = resample(sample, sr, meta.sample_rate)\n",
    "            sr = meta.sample_rate\n",
    "\n",
    "        noisy_wav = os.path.join(base_path, 'noisy_wav1558.wav')\n",
    "        save_audio(noisy_wav, sample, sr)\n",
    "        enhanced_wav = os.path.join(base_path, 'enhanced_wav1558.wav') \n",
    "        save_audio(enhanced_wav, enhanced, sr)\n",
    "        noisy_wav = os.path.join(base_path, 'dataset', 'dacon_dataset', dir_name, f'noisy_{id}.wav')\n",
    "        shutil.move(os.path.join(base_path, 'noisy_wav1558.wav'), noisy_wav)\n",
    "        enhanced_wav = os.path.join(base_path, 'dataset', 'dacon_dataset', dir_name, f'enhanced_{id}.wav')\n",
    "        shutil.move(os.path.join(base_path, 'enhanced_wav1558.wav') ,  enhanced_wav)\n",
    "    except:\n",
    "        print(f\"너무 짧아서 안됨: {id}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_list = [os.path.join(base_path, 'dataset', 'dacon_dataset', 'unlabeled_data_denoising', i) \\\n",
    "                   for i in os.listdir(os.path.join(base_path, 'dataset', 'dacon_dataset', 'unlabeled_data_denoising')) if \"noisy\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n",
      "100%|██████████| 1264/1264 [13:51<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All boundaries and audio lengths have been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from speechbrain.inference.VAD import VAD\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# VAD 모델 초기화\n",
    "vad_model = VAD.from_hparams(source=\"speechbrain/vad-crdnn-libriparty\", savedir=\"pretrained_models/vad-crdnn-libriparty\")\n",
    "\n",
    "# 오디오 파일 경로 리스트\n",
    "audio_file_list = [os.path.join(base_path, 'dataset', 'dacon_dataset', 'unlabeled_data_denoising', i) \\\n",
    "                   for i in os.listdir(os.path.join(base_path, 'dataset', 'dacon_dataset', 'unlabeled_data_denoising')) if \"noisy\" in i]\n",
    "\n",
    "\n",
    "# 타겟 샘플링 레이트\n",
    "target_sample_rate = 16000\n",
    "\n",
    "# 음성 구간 정보를 저장할 디렉토리\n",
    "output_dir = os.path.join(base_path, 'vad_bandstop_boundaries_unlabeled')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 오디오 파일 처리\n",
    "for input_file in tqdm(audio_file_list):\n",
    "    # Step 1: 오디오 파일 로드 및 샘플링 레이트 변환\n",
    "    signal, original_sample_rate = torchaudio.load(input_file)\n",
    "    transform = torchaudio.transforms.Resample(orig_freq=original_sample_rate, new_freq=target_sample_rate)\n",
    "    resampled_signal = transform(signal)\n",
    "    \n",
    "    # 오디오 길이 계산 (초 단위)\n",
    "    audio_length = resampled_signal.size(1) / target_sample_rate\n",
    "    \n",
    "    # 임시 파일로 저장\n",
    "    temp_output_file = os.path.join(base_path, 'temp_resampled_audio_file3.wav')\n",
    "    torchaudio.save(temp_output_file, resampled_signal, target_sample_rate)\n",
    "    \n",
    "    # Step 2: 변환된 파일로 VAD 수행\n",
    "    boundaries = vad_model.get_speech_segments(temp_output_file)\n",
    "    \n",
    "    # Step 3: 각 음성 구간의 시작 시간과 종료 시간을 기록\n",
    "    base_filename = os.path.splitext(os.path.basename(input_file))[0]\n",
    "    boundaries_list = []\n",
    "    for boundary in boundaries:\n",
    "        start, end = boundary[0].item(), boundary[1].item()\n",
    "        boundaries_list.append({\n",
    "            \"start\": start,\n",
    "            \"end\": end\n",
    "        })\n",
    "    \n",
    "    # JSON 파일로 저장\n",
    "    output_data = {\n",
    "        \"audio_length\": audio_length,\n",
    "        \"boundaries\": boundaries_list\n",
    "    }\n",
    "    boundaries_filename = f\"{base_filename}_boundaries.json\"\n",
    "    boundaries_filepath = os.path.join(output_dir, boundaries_filename)\n",
    "    with open(boundaries_filepath, 'w') as f:\n",
    "        json.dump(output_data, f)\n",
    "    \n",
    "    # print(f\"Saved boundaries and audio length for {input_file} to {boundaries_filepath}\")\n",
    "    \n",
    "    # 임시 파일 삭제\n",
    "    os.remove(temp_output_file)\n",
    "\n",
    "print(\"All boundaries and audio lengths have been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files: 100%|██████████| 1264/1264 [00:07<00:00, 165.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 경로 설정\n",
    "json_dir = os.path.join(base_path, 'vad_bandstop_boundaries_unlabeled')\n",
    "audio_dir = os.path.join(base_path, 'dataset', 'dacon_dataset', 'unlabeled_data')\n",
    "output_dir = os.path.join(base_path, 'dataset', 'dacon_dataset', 'unlabeled_data_silent')\n",
    "\n",
    "\n",
    "# 출력 디렉토리 생성\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# JSON 파일 목록 가져오기\n",
    "json_files = [f for f in os.listdir(json_dir) if f.endswith('_boundaries.json')]\n",
    "\n",
    "# JSON 파일 처리\n",
    "for json_file in tqdm(json_files, desc=\"Processing JSON files\"):\n",
    "    name = json_file.replace('_boundaries.json', '').replace('noisy_', '')\n",
    "    \n",
    "    \n",
    "    # JSON 파일 경로\n",
    "    json_path = os.path.join(json_dir, json_file)\n",
    "    \n",
    "    # 오디오 파일 경로\n",
    "    audio_path = os.path.join(audio_dir, f\"{name}.ogg\")\n",
    "    \n",
    "    # JSON 파일 읽기\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 오디오 파일 읽기\n",
    "    audio_data, sr = librosa.load(audio_path, sr=None)\n",
    "    \n",
    "    # boundaries 가져오기\n",
    "    boundaries = data.get('boundaries', [])\n",
    "    \n",
    "    # 간격 설정 (여기서는 0.1초를 추가로 잘라냅니다)\n",
    "    margin = 0.1  # 100 milliseconds\n",
    "    \n",
    "    if not boundaries:\n",
    "        # boundaries가 없으면 전체 오디오 데이터 사용\n",
    "        cropped_audio = audio_data\n",
    "    else:\n",
    "        # 제외할 구간 설정\n",
    "        exclude_intervals = [(max(boundary['start'] - margin, 0), min(boundary['end'] + margin, len(audio_data) / sr)) for boundary in boundaries]\n",
    "        \n",
    "        # 제외할 구간을 제외한 오디오 데이터 생성\n",
    "        keep_samples = []\n",
    "        prev_end = 0.0\n",
    "        for start, end in exclude_intervals:\n",
    "            if prev_end < start:\n",
    "                keep_samples.append(audio_data[int(prev_end * sr):int(start * sr)])\n",
    "            prev_end = end\n",
    "        if prev_end * sr < len(audio_data):\n",
    "            keep_samples.append(audio_data[int(prev_end * sr):])\n",
    "        \n",
    "        # 모든 유지할 샘플들을 합침\n",
    "        cropped_audio = np.concatenate(keep_samples) if keep_samples else np.array([], dtype=audio_data.dtype)\n",
    "    \n",
    "    # 결과 오디오 파일 저장\n",
    "    output_audio_path = os.path.join(output_dir, f\"cropped_{name}.wav\")\n",
    "    sf.write(output_audio_path, cropped_audio, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging audio files: 100%|██████████| 2528/2528 [00:00<00:00, 4503.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged audio file saved at /home/hyj/ChanHyung/Audio/DACON_fake_voice_detection/merged_audio2.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 경로 설정\n",
    "output_dir = os.path.join(base_path, 'dataset', 'dacon_dataset', 'unlabeled_data_silent')\n",
    "merged_output_path = os.path.join(base_path, 'merged_audio2.wav')\n",
    "\n",
    "\n",
    "# 병합할 오디오 파일 목록 가져오기\n",
    "audio_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith('.wav')]\n",
    "\n",
    "# 병합할 오디오 데이터를 저장할 리스트 초기화\n",
    "merged_audio_data = []\n",
    "sample_rate = None\n",
    "\n",
    "# 각 오디오 파일을 읽고 병합\n",
    "for audio_file in tqdm(audio_files, desc=\"Merging audio files\"):\n",
    "    audio_data, sr = sf.read(audio_file)\n",
    "    if sample_rate is None:\n",
    "        sample_rate = sr  # 첫 번째 오디오 파일의 샘플링 레이트 사용\n",
    "    else:\n",
    "        assert sample_rate == sr, f\"Sample rate mismatch: {audio_file}\"\n",
    "    merged_audio_data.append(audio_data)\n",
    "\n",
    "# 병합된 오디오 데이터를 하나의 배열로 결합\n",
    "if merged_audio_data:\n",
    "    merged_audio_data = np.concatenate(merged_audio_data)\n",
    "    # 병합된 오디오 데이터를 새로운 파일로 저장\n",
    "    sf.write(merged_output_path, merged_audio_data, sample_rate)\n",
    "    print(f\"Merged audio file saved at {merged_output_path}\")\n",
    "else:\n",
    "    print(\"No audio files to merge.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio segments: 100%|█████████▉| 259/260 [00:00<00:00, 288.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 이 부분은 skip\n",
    "# 00:19-00:20\n",
    "# 00:30-00:37\n",
    "# 14:23-14:25\n",
    "# 28:12-28:13\n",
    "# 29:00-29:04\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 경로 설정\n",
    "input_audio_path = os.path.join(base_path, 'merged_audio2.wav')\n",
    "output_dir = os.path.join(base_path, 'dataset', 'dacon_dataset', 'unlabeled_data_silent_split')\n",
    "\n",
    "# 출력 디렉토리 생성\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 오디오 파일 읽기\n",
    "processed_audio, sr = librosa.load(input_audio_path, sr=None)\n",
    "\n",
    "noisy_datasets = []\n",
    "\n",
    "i=0\n",
    "\n",
    "# 5초씩 자르기\n",
    "start = 0\n",
    "segment_count = 0\n",
    "for start in tqdm(range(0, len(processed_audio), 5 * sr), desc=\"Processing audio segments\"):\n",
    "    end = start + 5 * sr\n",
    "    if end <= len(processed_audio):\n",
    "        segment = processed_audio[start:end]\n",
    "        output_audio_path = os.path.join(output_dir, f\"UNLABELED_{i:05d}.wav\")\n",
    "        sf.write(output_audio_path, segment, sr)\n",
    "        segment_count += 1\n",
    "\n",
    "        i += 1\n",
    "        noisy_datasets.append({\n",
    "                'id1': audio_path,\n",
    "                'id2': \"\",\n",
    "                'path': output_audio_path,\n",
    "                'fake': 0,\n",
    "                'real': 0\n",
    "            })\n",
    "    else:\n",
    "        # 마지막 segment가 5초 미만이면 저장하지 않음\n",
    "        break\n",
    "\n",
    "print(\"Processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "noisy_datasets = pd.DataFrame(noisy_datasets)\n",
    "noisy_datasets.to_csv(os.path.join(base_path, 'dataset', 'dacon_dataset', 'noisy_datasets.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239971 50000\n"
     ]
    }
   ],
   "source": [
    "base_path = os.getcwd()\n",
    "train_df = pd.read_csv(os.path.join(base_path, 'dataset', 'dacon_dataset', \"train_bandstop_new_dataset_addmix.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(base_path, 'dataset', 'dacon_dataset', \"test.csv\"))\n",
    "\n",
    "x_train_zero = train_df['path'].values.tolist()[-384:] * 156\n",
    "y_train_zero = train_df[['fake', 'real']][-384:].values.tolist() * 156\n",
    "\n",
    "x_train = train_df['path'][:-384].values.tolist() + x_train_zero\n",
    "y_train = train_df[['fake', 'real']][:-384].values.tolist() + y_train_zero\n",
    "\n",
    "x_test = [os.path.join(base_path, 'dataset', 'dacon_dataset', \"test_bandstop_denoising\", f\"enhanced_{i}.wav\") for i in test_df['id'].tolist()]\n",
    "\n",
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    (1, 0): 0,\n",
    "    (0, 1): 1,\n",
    "    (2, 0): 2,\n",
    "    (0, 2): 3,\n",
    "    (1, 1): 4,\n",
    "    (0, 0): 5\n",
    "}\n",
    "\n",
    "y_train_transformed = [label_mapping[tuple(labels)] for labels in y_train]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 1200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, X_val, _, y_val = train_test_split(x_train, y_train_transformed, test_size=0.005, stratify=y_train_transformed, random_state=42)\n",
    "\n",
    "print(f\"val: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_zero_datasets = [os.path.join(base_path, 'dataset', 'dacon_dataset', 'unlabeled_data_silent_split', f'{i}') for i in os.listdir(os.path.join(base_path, 'dataset', 'dacon_dataset', 'unlabeled_data_silent_split'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import soundfile as sf\n",
    "\n",
    "def pad_and_randomize_audio(audio, sr=16000, target_length=5):\n",
    "    \"\"\"5초 이하의 음성을 5초 길이로 패딩하고 랜덤 위치에 배치\"\"\"\n",
    "    target_samples = sr * target_length\n",
    "    audio_length = len(audio)\n",
    "\n",
    "    if audio_length >= target_samples:\n",
    "        return audio[:target_samples]  # 이미 5초 이상인 경우, 잘라서 반환\n",
    "\n",
    "    # 5초 이하인 경우\n",
    "    padded_audio = np.zeros(target_samples)\n",
    "    start_pos = random.randint(0, target_samples - audio_length)\n",
    "    padded_audio[start_pos:start_pos + audio_length] = audio\n",
    "    return padded_audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:18<00:00, 66.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "import librosa\n",
    "import random\n",
    "\n",
    "output_dir = os.path.join(base_path, 'dataset', 'dacon_dataset', 'valid')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "val_datasets = []\n",
    "\n",
    "for i, (path, label) in tqdm(enumerate(zip(X_val, y_val)), total=len(X_val)):\n",
    "    audio, sr = librosa.load(path)\n",
    "    padded_audio = pad_and_randomize_audio(audio, sr, target_length=5)\n",
    "\n",
    "    noisy_audio, noisy_sr = librosa.load(random.choice(zero_zero_datasets))\n",
    "    audio = librosa.resample(padded_audio, orig_sr=sr, target_sr=noisy_sr)\n",
    "    noisy_audio = noisy_audio[:noisy_sr*5]\n",
    "    combined_audio = noisy_audio + audio\n",
    "\n",
    "    noisy_audio, noisy_sr = librosa.load(random.choice(zero_zero_datasets))\n",
    "    noisy_audio = noisy_audio[:noisy_sr*5]\n",
    "    combined_audio += noisy_audio\n",
    "    \n",
    "    if random.random() < 0.7:\n",
    "        noisy_audio, noisy_sr = librosa.load(random.choice(zero_zero_datasets))\n",
    "        noisy_audio = noisy_audio[:noisy_sr*5]\n",
    "        combined_audio += noisy_audio\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"VALID_{i:05d}.wav\")\n",
    "    val_datasets.append({\"path\": output_path, \"label\": label})\n",
    "    sf.write(output_path, combined_audio, noisy_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame(val_datasets)\n",
    "val_df.to_csv(os.path.join(base_path, 'valid.csv'), index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torchaudio.backend.common import AudioMetaData\n",
    "\n",
    "from df import config\n",
    "from df.enhance import enhance, init_df, load_audio, save_audio\n",
    "from df.io import resample\n",
    "\n",
    "from torchaudio.backend.common import AudioMetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 11:37:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mLoading model settings of DeepFilterNet2\u001b[0m\n",
      "\u001b[32m2024-07-23 11:37:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mInitializing model `deepfilternet2`\u001b[0m\n",
      "\u001b[32m2024-07-23 11:37:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mFound checkpoint ./DeepFilterNet2/checkpoints/model_96.ckpt.best with epoch 96\u001b[0m\n",
      "\u001b[32m2024-07-23 11:37:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on device cuda:0\u001b[0m\n",
      "\u001b[32m2024-07-23 11:37:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mModel loaded\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607044abce2246609edfe8f085104da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import shutil\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "dir_name = 'valid_denoising'\n",
    "output_dir = os.path.join(base_path, 'dataset', 'dacon_dataset', dir_name)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model, df, _ = init_df(\"./DeepFilterNet2\", config_allow_defaults=True)\n",
    "model = model.to(device=device).eval()\n",
    "train_x = os.listdir(os.path.join(base_path, 'dataset', 'dacon_dataset', 'valid'))\n",
    "\n",
    "\n",
    "for data in tqdm(val_datasets):\n",
    "    id = data['path'].split('/')[-1].split('.')[0]\n",
    "    sr = config(\"sr\", 32000, int, section=\"df\")\n",
    "    sample, meta = load_audio(data['path'], sr)\n",
    "    enhanced = enhance(model, df, sample)\n",
    "\n",
    "    lim = torch.linspace(0.0, 1.0, int(sr * 0.15)).unsqueeze(0)\n",
    "    lim = torch.cat((lim, torch.ones(1, enhanced.shape[1] - lim.shape[1])), dim=1)\n",
    "    enhanced = enhanced * lim\n",
    "    if meta.sample_rate != sr:\n",
    "        enhanced = resample(enhanced, sr, meta.sample_rate)\n",
    "        sample = resample(sample, sr, meta.sample_rate)\n",
    "        sr = meta.sample_rate\n",
    "\n",
    "    noisy_wav = \"./noisy_wav1551.wav\"\n",
    "    save_audio(noisy_wav, sample, sr)\n",
    "    enhanced_wav = \"./enhanced_wav1551.wav\"\n",
    "    save_audio(enhanced_wav, enhanced, sr)\n",
    "    noisy_wav = os.path.join(base_path, 'dataset', 'dacon_dataset', dir_name, f\"noisy_{id}.wav\")\n",
    "    shutil.move(\"./noisy_wav1551.wav\", noisy_wav)\n",
    "    enhanced_wav = os.path.join(base_path, 'dataset', 'dacon_dataset', dir_name, f\"enhanced_{id}.wav\")\n",
    "    shutil.move(\"./enhanced_wav1551.wav\",  enhanced_wav)\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cHb_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
