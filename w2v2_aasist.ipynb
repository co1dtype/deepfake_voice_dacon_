{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoModelForAudioClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torchaudio.backend.common import AudioMetaData\n",
    "\n",
    "from df import config\n",
    "from df.enhance import enhance, init_df, load_audio, save_audio\n",
    "from df.io import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'SR':16_000,\n",
    "    'SEED':42,\n",
    "    'BATCH_SIZE':24, \n",
    "    'EPOCHS':20,\n",
    "    'LR':1e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239971\n"
     ]
    }
   ],
   "source": [
    "base_path = os.getcwd()\n",
    "train_df = pd.read_csv(f'{base_path}/dataset/dacon_dataset/train_bandstop_new_dataset_addmix.csv')\n",
    "test_df = pd.read_csv(f'{base_path}/dataset/dacon_dataset/test.csv')\n",
    "\n",
    "x_train_zero = train_df['path'].values.tolist()[-384:] * 156\n",
    "y_train_zero = train_df[['fake', 'real']][-384:].values.tolist() * 156\n",
    "\n",
    "x_train = train_df['path'][:-384].values.tolist() + x_train_zero\n",
    "y_train = train_df[['fake', 'real']][:-384].values.tolist() + y_train_zero\n",
    "\n",
    "\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    (1, 0): 0,\n",
    "    (0, 1): 1,\n",
    "    (2, 0): 2,\n",
    "    (0, 2): 3,\n",
    "    (1, 1): 4,\n",
    "    (0, 0): 5\n",
    "}\n",
    "\n",
    "y_train_transformed = [label_mapping[tuple(labels)] for labels in y_train]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 75570 val: 8397 test: 50000\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_transformed, test_size=0.1, stratify=y_train_transformed, random_state=CFG['SEED'])\n",
    "_, X_val, _, y_val = train_test_split(x_train, y_train_transformed, test_size=0.35, stratify=y_train_transformed, random_state=CFG['SEED'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_val, y_val, test_size=0.1, stratify=y_val, random_state=CFG['SEED'])\n",
    "\n",
    "print(f\"train: {len(X_train)} val: {len(X_val)} test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPitchShiftSegment:\n",
    "    def __init__(self, min_semitones=-12, max_semitones=2, duration_range=(1, 5), fade_duration=0.1, p=0.5):\n",
    "        self.min_semitones = min_semitones\n",
    "        self.max_semitones = max_semitones\n",
    "        self.duration_range = duration_range\n",
    "        self.fade_duration = fade_duration\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, samples, sample_rate):\n",
    "        if random.random() > self.p:\n",
    "            return samples\n",
    "        \n",
    "        total_duration = len(samples) / sample_rate\n",
    "        segment_duration = random.uniform(*self.duration_range)\n",
    "        fade_samples = int(self.fade_duration * sample_rate)\n",
    "        \n",
    "        start_time = random.uniform(0, total_duration - segment_duration)\n",
    "        start_sample = int(start_time * sample_rate)\n",
    "        end_sample = int((start_time + segment_duration) * sample_rate)\n",
    "        \n",
    "        pitch_shift = PitchShift(min_semitones=self.min_semitones, max_semitones=self.max_semitones, p=1.0)\n",
    "        original_segment = samples[start_sample:end_sample].copy()\n",
    "        shifted_segment = pitch_shift(samples=original_segment, sample_rate=sample_rate)\n",
    "        \n",
    "        # Apply crossfade\n",
    "        for i in range(fade_samples):\n",
    "            fade_in_factor = i / fade_samples\n",
    "            fade_out_factor = 1 - fade_in_factor\n",
    "            shifted_segment[i] = fade_out_factor * original_segment[i] + fade_in_factor * shifted_segment[i]\n",
    "            shifted_segment[-i-1] = fade_in_factor * original_segment[-i-1] + fade_out_factor * shifted_segment[-i-1]\n",
    "        \n",
    "        samples[start_sample:end_sample] = shifted_segment\n",
    "        \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(\"./valid.csv\")\n",
    "y_val = val_df['label'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = [f\"./dataset/dacon_dataset/valid_denoising/enhanced_VALID_{i:05d}.wav\" for i in range(len(y_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'facebook/wav2vec2-xls-r-300m'\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def pad_and_randomize_audio(audio, sr=16000, target_length=5):\n",
    "    \"\"\"5초 이하의 음성을 5초 길이로 패딩하고 랜덤 위치에 배치\"\"\"\n",
    "    target_samples = sr * target_length\n",
    "    audio_length = len(audio)\n",
    "\n",
    "    if audio_length >= target_samples:\n",
    "        return audio[:target_samples]  # 이미 5초 이상인 경우, 잘라서 반환\n",
    "\n",
    "    # 5초 이하인 경우\n",
    "    padded_audio = np.zeros(target_samples)\n",
    "    start_pos = random.randint(0, target_samples - audio_length)\n",
    "    padded_audio[start_pos:start_pos + audio_length] = audio\n",
    "    return padded_audio\n",
    "\n",
    "\n",
    "from audiomentations import Compose, BandStopFilter, AirAbsorption\n",
    "\n",
    "# 증강 기법 정의 예시\n",
    "augment = Compose([\n",
    "    BandStopFilter(min_center_freq=200.0, max_center_freq=8000.0, min_bandwidth_fraction=0.1, max_bandwidth_fraction=0.4, p=1.0)\n",
    "])\n",
    "\n",
    "# 배치 데이터를 처리하는 함수\n",
    "def process_batch(file_paths, is_augment=False):\n",
    "    audio_batch = []\n",
    "    for audio_path in tqdm(file_paths):\n",
    "        audio, sr = librosa.load(audio_path)\n",
    "        if is_augment:\n",
    "            audio = augment(audio[:sr*5], sample_rate=sr)\n",
    "        audio = pad_and_randomize_audio(audio, sr=sr, target_length=5)\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "        # audio = processor(audio, sampling_rate=16000, return_tensors=\"np\", padding=True).input_values\n",
    "        audio_batch.append(audio)\n",
    "\n",
    "    audio_batch = np.vstack(audio_batch)\n",
    "    return audio_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8d29725104446dbf0ff24918cb48b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_x \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_augment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m valid_x \u001b[38;5;241m=\u001b[39m process_batch(X_val, is_augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[16], line 32\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(file_paths, is_augment)\u001b[0m\n\u001b[1;32m     30\u001b[0m audio, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(audio_path)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_augment:\n\u001b[0;32m---> 32\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m audio \u001b[38;5;241m=\u001b[39m pad_and_randomize_audio(audio, sr\u001b[38;5;241m=\u001b[39msr, target_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     34\u001b[0m audio \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mresample(audio, orig_sr\u001b[38;5;241m=\u001b[39msr, target_sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/DACON_DEEPFAKE/lib/python3.11/site-packages/audiomentations/core/composition.py:91\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, samples, sample_rate)\u001b[0m\n\u001b[1;32m     89\u001b[0m         random\u001b[38;5;241m.\u001b[39mshuffle(transforms)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m transforms:\n\u001b[0;32m---> 91\u001b[0m         samples \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples\n",
      "File \u001b[0;32m~/anaconda3/envs/DACON_DEEPFAKE/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:93\u001b[0m, in \u001b[0;36mBaseWaveformTransform.__call__\u001b[0;34m(self, samples, sample_rate)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_mono:\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MonoAudioNotSupportedException(\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m only supports multichannel audio, not mono audio\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     90\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m     91\u001b[0m             )\n\u001b[1;32m     92\u001b[0m         )\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples\n",
      "File \u001b[0;32m~/anaconda3/envs/DACON_DEEPFAKE/lib/python3.11/site-packages/audiomentations/augmentations/base_butterword_filter.py:226\u001b[0m, in \u001b[0;36mBaseButterworthFilter.apply\u001b[0;34m(self, samples, sample_rate)\u001b[0m\n\u001b[1;32m    224\u001b[0m         processed_samples \u001b[38;5;241m=\u001b[39m sosfiltfilt(sos, samples)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m         processed_samples, _ \u001b[38;5;241m=\u001b[39m \u001b[43msosfilt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m            \u001b[49m\u001b[43msos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msosfilt_zi\u001b[49m\u001b[43m(\u001b[49m\u001b[43msos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     processed_samples \u001b[38;5;241m=\u001b[39m processed_samples\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/DACON_DEEPFAKE/lib/python3.11/site-packages/scipy/signal/_signaltools.py:4344\u001b[0m, in \u001b[0;36msosfilt\u001b[0;34m(sos, x, axis, zi)\u001b[0m\n\u001b[1;32m   4342\u001b[0m zi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(np\u001b[38;5;241m.\u001b[39mreshape(zi, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_sections, \u001b[38;5;241m2\u001b[39m)))\n\u001b[1;32m   4343\u001b[0m sos \u001b[38;5;241m=\u001b[39m sos\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 4344\u001b[0m \u001b[43m_sosfilt\u001b[49m\u001b[43m(\u001b[49m\u001b[43msos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4345\u001b[0m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m x_shape\n\u001b[1;32m   4346\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmoveaxis(x, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, axis)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_x = process_batch(X_train, is_augment=True)\n",
    "valid_x = process_batch(X_val, is_augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0_and_2: 18900 occurrences\n",
      "Label 1_and_3: 18900 occurrences\n",
      "Label 4: 18900 occurrences\n",
      "Label 5: 18870 occurrences\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(y_train)\n",
    "\n",
    "combined_counts = {\n",
    "    '0_and_2': label_counts[0] + label_counts[2],\n",
    "    '1_and_3': label_counts[1] + label_counts[3],\n",
    "    '4': label_counts[4],\n",
    "    '5': label_counts[5]\n",
    "}\n",
    "\n",
    "# 합산된 라벨 균형을 출력\n",
    "for label, count in combined_counts.items():\n",
    "    print(f\"Label {label}: {count} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2ten = {0: [1,0], 1:[0,1],\n",
    "           2: [1,0], 3:[0,1],\n",
    "           4: [1,1], 5:[0,0]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, OneOf\n",
    "\n",
    "class CustomDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y, train_mode=False, augment_func=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.train_mode = train_mode\n",
    "        self.augment_func = augment_func\n",
    "        self.augmentation = Compose([\n",
    "            OneOf([\n",
    "                # Compose([AddReverb(reverb_amount=0.6, p=1), AddEcho(delay=0.09, attenuation=0.5, sample_rate=32000, p=1)]),\n",
    "                # PitchShift(min_semitones=-10, max_semitones=2, p=1),\n",
    "                GainTransition(min_gain_in_db=-140, max_gain_in_db=0, p=1.),\n",
    "                RandomPitchShiftSegment(min_semitones=-10, max_semitones=2, duration_range=(1, 3.1), fade_duration=0.1, p=1.0),\n",
    "            ], p=0.35)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_values = self.x[idx]\n",
    "        \n",
    "        if self.y is not None:\n",
    "            labels = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "            labels_trans = torch.tensor(idx2ten[self.y[idx]], dtype=torch.float32)\n",
    "            if (labels == torch.tensor([5], dtype=torch.float32)).all():\n",
    "                input_values = self.augment_func(samples=input_values, sample_rate=16000)\n",
    "\n",
    "            if random.random() < 0.5 and self.train_mode:\n",
    "                input_values = self.augmentation(input_values, sample_rate=16000)            \n",
    "            input_values = processor(input_values, sampling_rate=16000, return_tensors=\"pt\", padding=True).input_values.squeeze()\n",
    "\n",
    "            return input_values, labels, labels_trans\n",
    "        else:\n",
    "            input_values = processor(input_values, sampling_rate=16000, return_tensors=\"pt\", padding=True).input_values.squeeze()\n",
    "            return input_values\n",
    "\n",
    "# Define the augmentation pipeline using audiomentations\n",
    "augmentations = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "    Shift(min_shift=-0.5, max_shift=0.5, p=0.5)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(dataset, batch_size, shuffle, num_workers=0):\n",
    "    return DataLoader(dataset,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      num_workers=num_workers,\n",
    "                      pin_memory=True\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = CustomDataSet(train_x, y_train, train_mode=True, augment_func=augmentations)\n",
    "valid_dataset = CustomDataSet(valid_x, y_val, augment_func=augmentations)\n",
    "\n",
    "train_loader = create_data_loader(train_dataset, CFG['BATCH_SIZE'], True, 4)\n",
    "valid_loader = create_data_loader(valid_dataset, CFG['BATCH_SIZE'], False, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 80000]) torch.Size([24]) torch.Size([24, 2])\n",
      "tensor([4., 5., 4., 5., 5., 5., 1., 4., 0., 5., 5., 2., 5., 4., 0., 1., 5., 5.,\n",
      "        0., 4., 1., 0., 5., 4.])\n"
     ]
    }
   ],
   "source": [
    "data, target, target_trans = next(iter(train_loader))\n",
    "print(data.shape, target.shape, target_trans.shape)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AASIST\n",
    "Copyright (c) 2021-present NAVER Corp.\n",
    "MIT license\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # attention map\n",
    "        self.att_proj = nn.Linear(in_dim, out_dim)\n",
    "        self.att_weight = self._init_new_params(out_dim, 1)\n",
    "\n",
    "        # project\n",
    "        self.proj_with_att = nn.Linear(in_dim, out_dim)\n",
    "        self.proj_without_att = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        # batch norm\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "\n",
    "        # dropout for inputs\n",
    "        self.input_drop = nn.Dropout(p=0.2)\n",
    "\n",
    "        # activate\n",
    "        self.act = nn.SELU(inplace=True)\n",
    "\n",
    "        # temperature\n",
    "        self.temp = 1.\n",
    "        if \"temperature\" in kwargs:\n",
    "            self.temp = kwargs[\"temperature\"]\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x   :(#bs, #node, #dim)\n",
    "        '''\n",
    "        # apply input dropout\n",
    "        x = self.input_drop(x)\n",
    "\n",
    "        # derive attention map\n",
    "        att_map = self._derive_att_map(x)\n",
    "\n",
    "        # projection\n",
    "        x = self._project(x, att_map)\n",
    "\n",
    "        # apply batch norm\n",
    "        x = self._apply_BN(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "    def _pairwise_mul_nodes(self, x):\n",
    "        '''\n",
    "        Calculates pairwise multiplication of nodes.\n",
    "        - for attention map\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, #dim)\n",
    "        '''\n",
    "\n",
    "        nb_nodes = x.size(1)\n",
    "        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n",
    "        x_mirror = x.transpose(1, 2)\n",
    "\n",
    "        return x * x_mirror\n",
    "\n",
    "    def _derive_att_map(self, x):\n",
    "        '''\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, 1)\n",
    "        '''\n",
    "        att_map = self._pairwise_mul_nodes(x)\n",
    "        # size: (#bs, #node, #node, #dim_out)\n",
    "        att_map = torch.tanh(self.att_proj(att_map))\n",
    "        # size: (#bs, #node, #node, 1)\n",
    "        att_map = torch.matmul(att_map, self.att_weight)\n",
    "\n",
    "        # apply temperature\n",
    "        att_map = att_map / self.temp\n",
    "\n",
    "        att_map = F.softmax(att_map, dim=-2)\n",
    "\n",
    "        return att_map\n",
    "\n",
    "    def _project(self, x, att_map):\n",
    "        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n",
    "        x2 = self.proj_without_att(x)\n",
    "\n",
    "        return x1 + x2\n",
    "\n",
    "    def _apply_BN(self, x):\n",
    "        org_size = x.size()\n",
    "        x = x.view(-1, org_size[-1])\n",
    "        x = self.bn(x)\n",
    "        x = x.view(org_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _init_new_params(self, *size):\n",
    "        out = nn.Parameter(torch.FloatTensor(*size))\n",
    "        nn.init.xavier_normal_(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class HtrgGraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj_type1 = nn.Linear(in_dim, in_dim)\n",
    "        self.proj_type2 = nn.Linear(in_dim, in_dim)\n",
    "\n",
    "        # attention map\n",
    "        self.att_proj = nn.Linear(in_dim, out_dim)\n",
    "        self.att_projM = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        self.att_weight11 = self._init_new_params(out_dim, 1)\n",
    "        self.att_weight22 = self._init_new_params(out_dim, 1)\n",
    "        self.att_weight12 = self._init_new_params(out_dim, 1)\n",
    "        self.att_weightM = self._init_new_params(out_dim, 1)\n",
    "\n",
    "        # project\n",
    "        self.proj_with_att = nn.Linear(in_dim, out_dim)\n",
    "        self.proj_without_att = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        self.proj_with_attM = nn.Linear(in_dim, out_dim)\n",
    "        self.proj_without_attM = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        # batch norm\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "\n",
    "        # dropout for inputs\n",
    "        self.input_drop = nn.Dropout(p=0.2)\n",
    "\n",
    "        # activate\n",
    "        self.act = nn.SELU(inplace=True)\n",
    "\n",
    "        # temperature\n",
    "        self.temp = 1.\n",
    "        if \"temperature\" in kwargs:\n",
    "            self.temp = kwargs[\"temperature\"]\n",
    "\n",
    "    def forward(self, x1, x2, master=None):\n",
    "        '''\n",
    "        x1  :(#bs, #node, #dim)\n",
    "        x2  :(#bs, #node, #dim)\n",
    "        '''\n",
    "        num_type1 = x1.size(1)\n",
    "        num_type2 = x2.size(1)\n",
    "\n",
    "        x1 = self.proj_type1(x1)\n",
    "        x2 = self.proj_type2(x2)\n",
    "\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "\n",
    "        if master is None:\n",
    "            master = torch.mean(x, dim=1, keepdim=True)\n",
    "\n",
    "        # apply input dropout\n",
    "        x = self.input_drop(x)\n",
    "\n",
    "        # derive attention map\n",
    "        att_map = self._derive_att_map(x, num_type1, num_type2)\n",
    "\n",
    "        # directional edge for master node\n",
    "        master = self._update_master(x, master)\n",
    "\n",
    "        # projection\n",
    "        x = self._project(x, att_map)\n",
    "\n",
    "        # apply batch norm\n",
    "        x = self._apply_BN(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x1 = x.narrow(1, 0, num_type1)\n",
    "        x2 = x.narrow(1, num_type1, num_type2)\n",
    "\n",
    "        return x1, x2, master\n",
    "\n",
    "    def _update_master(self, x, master):\n",
    "\n",
    "        att_map = self._derive_att_map_master(x, master)\n",
    "        master = self._project_master(x, master, att_map)\n",
    "\n",
    "        return master\n",
    "\n",
    "    def _pairwise_mul_nodes(self, x):\n",
    "        '''\n",
    "        Calculates pairwise multiplication of nodes.\n",
    "        - for attention map\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, #dim)\n",
    "        '''\n",
    "\n",
    "        nb_nodes = x.size(1)\n",
    "        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n",
    "        x_mirror = x.transpose(1, 2)\n",
    "\n",
    "        return x * x_mirror\n",
    "\n",
    "    def _derive_att_map_master(self, x, master):\n",
    "        '''\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, 1)\n",
    "        '''\n",
    "        att_map = x * master\n",
    "        att_map = torch.tanh(self.att_projM(att_map))\n",
    "\n",
    "        att_map = torch.matmul(att_map, self.att_weightM)\n",
    "\n",
    "        # apply temperature\n",
    "        att_map = att_map / self.temp\n",
    "\n",
    "        att_map = F.softmax(att_map, dim=-2)\n",
    "\n",
    "        return att_map\n",
    "\n",
    "    def _derive_att_map(self, x, num_type1, num_type2):\n",
    "        '''\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, 1)\n",
    "        '''\n",
    "        att_map = self._pairwise_mul_nodes(x)\n",
    "        # size: (#bs, #node, #node, #dim_out)\n",
    "        att_map = torch.tanh(self.att_proj(att_map))\n",
    "        # size: (#bs, #node, #node, 1)\n",
    "\n",
    "        att_board = torch.zeros_like(att_map[:, :, :, 0]).unsqueeze(-1)\n",
    "\n",
    "        att_board[:, :num_type1, :num_type1, :] = torch.matmul(\n",
    "            att_map[:, :num_type1, :num_type1, :], self.att_weight11)\n",
    "        att_board[:, num_type1:, num_type1:, :] = torch.matmul(\n",
    "            att_map[:, num_type1:, num_type1:, :], self.att_weight22)\n",
    "        att_board[:, :num_type1, num_type1:, :] = torch.matmul(\n",
    "            att_map[:, :num_type1, num_type1:, :], self.att_weight12)\n",
    "        att_board[:, num_type1:, :num_type1, :] = torch.matmul(\n",
    "            att_map[:, num_type1:, :num_type1, :], self.att_weight12)\n",
    "\n",
    "        att_map = att_board\n",
    "\n",
    "        # att_map = torch.matmul(att_map, self.att_weight12)\n",
    "\n",
    "        # apply temperature\n",
    "        att_map = att_map / self.temp\n",
    "\n",
    "        att_map = F.softmax(att_map, dim=-2)\n",
    "\n",
    "        return att_map\n",
    "\n",
    "    def _project(self, x, att_map):\n",
    "        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n",
    "        x2 = self.proj_without_att(x)\n",
    "\n",
    "        return x1 + x2\n",
    "\n",
    "    def _project_master(self, x, master, att_map):\n",
    "\n",
    "        x1 = self.proj_with_attM(torch.matmul(\n",
    "            att_map.squeeze(-1).unsqueeze(1), x))\n",
    "        x2 = self.proj_without_attM(master)\n",
    "\n",
    "        return x1 + x2\n",
    "\n",
    "    def _apply_BN(self, x):\n",
    "        org_size = x.size()\n",
    "        x = x.view(-1, org_size[-1])\n",
    "        x = self.bn(x)\n",
    "        x = x.view(org_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _init_new_params(self, *size):\n",
    "        out = nn.Parameter(torch.FloatTensor(*size))\n",
    "        nn.init.xavier_normal_(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GraphPool(nn.Module):\n",
    "    def __init__(self, k: float, in_dim: int, p: Union[float, int]):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.drop = nn.Dropout(p=p) if p > 0 else nn.Identity()\n",
    "        self.in_dim = in_dim\n",
    "\n",
    "    def forward(self, h):\n",
    "        Z = self.drop(h)\n",
    "        weights = self.proj(Z)\n",
    "        scores = self.sigmoid(weights)\n",
    "        new_h = self.top_k_graph(scores, h, self.k)\n",
    "\n",
    "        return new_h\n",
    "\n",
    "    def top_k_graph(self, scores, h, k):\n",
    "        \"\"\"\n",
    "        args\n",
    "        =====\n",
    "        scores: attention-based weights (#bs, #node, 1)\n",
    "        h: graph data (#bs, #node, #dim)\n",
    "        k: ratio of remaining nodes, (float)\n",
    "\n",
    "        returns\n",
    "        =====\n",
    "        h: graph pool applied data (#bs, #node', #dim)\n",
    "        \"\"\"\n",
    "        _, n_nodes, n_feat = h.size()\n",
    "        n_nodes = max(int(n_nodes * k), 1)\n",
    "        _, idx = torch.topk(scores, n_nodes, dim=1)\n",
    "        idx = idx.expand(-1, -1, n_feat)\n",
    "\n",
    "        h = h * scores\n",
    "        h = torch.gather(h, 1, idx)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "class CONV(nn.Module):\n",
    "    @staticmethod\n",
    "    def to_mel(hz):\n",
    "        return 2595 * np.log10(1 + hz / 700)\n",
    "\n",
    "    @staticmethod\n",
    "    def to_hz(mel):\n",
    "        return 700 * (10**(mel / 2595) - 1)\n",
    "\n",
    "    def __init__(self,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 sample_rate=16000,\n",
    "                 in_channels=1,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 dilation=1,\n",
    "                 bias=False,\n",
    "                 groups=1,\n",
    "                 mask=False):\n",
    "        super().__init__()\n",
    "        if in_channels != 1:\n",
    "\n",
    "            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (\n",
    "                in_channels)\n",
    "            raise ValueError(msg)\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n",
    "        if kernel_size % 2 == 0:\n",
    "            self.kernel_size = self.kernel_size + 1\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.mask = mask\n",
    "        if bias:\n",
    "            raise ValueError('SincConv does not support bias.')\n",
    "        if groups > 1:\n",
    "            raise ValueError('SincConv does not support groups.')\n",
    "\n",
    "        NFFT = 512\n",
    "        f = int(self.sample_rate / 2) * np.linspace(0, 1, int(NFFT / 2) + 1)\n",
    "        fmel = self.to_mel(f)\n",
    "        fmelmax = np.max(fmel)\n",
    "        fmelmin = np.min(fmel)\n",
    "        filbandwidthsmel = np.linspace(fmelmin, fmelmax, self.out_channels + 1)\n",
    "        filbandwidthsf = self.to_hz(filbandwidthsmel)\n",
    "\n",
    "        self.mel = filbandwidthsf\n",
    "        self.hsupp = torch.arange(-(self.kernel_size - 1) / 2,\n",
    "                                  (self.kernel_size - 1) / 2 + 1)\n",
    "        self.band_pass = torch.zeros(self.out_channels, self.kernel_size)\n",
    "        for i in range(len(self.mel) - 1):\n",
    "            fmin = self.mel[i]\n",
    "            fmax = self.mel[i + 1]\n",
    "            hHigh = (2*fmax/self.sample_rate) * \\\n",
    "                np.sinc(2*fmax*self.hsupp/self.sample_rate)\n",
    "            hLow = (2*fmin/self.sample_rate) * \\\n",
    "                np.sinc(2*fmin*self.hsupp/self.sample_rate)\n",
    "            hideal = hHigh - hLow\n",
    "\n",
    "            self.band_pass[i, :] = Tensor(np.hamming(\n",
    "                self.kernel_size)) * Tensor(hideal)\n",
    "\n",
    "    def forward(self, x, mask=False):\n",
    "        band_pass_filter = self.band_pass.clone().to(x.device)\n",
    "        if mask:\n",
    "            A = np.random.uniform(0, 20)\n",
    "            A = int(A)\n",
    "            A0 = random.randint(0, band_pass_filter.shape[0] - A)\n",
    "            band_pass_filter[A0:A0 + A, :] = 0\n",
    "        else:\n",
    "            band_pass_filter = band_pass_filter\n",
    "\n",
    "        self.filters = (band_pass_filter).view(self.out_channels, 1,\n",
    "                                               self.kernel_size)\n",
    "\n",
    "        return F.conv1d(x,\n",
    "                        self.filters,\n",
    "                        stride=self.stride,\n",
    "                        padding=self.padding,\n",
    "                        dilation=self.dilation,\n",
    "                        bias=None,\n",
    "                        groups=1)\n",
    "\n",
    "\n",
    "class Residual_block(nn.Module):\n",
    "    def __init__(self, nb_filts, first=False):\n",
    "        super().__init__()\n",
    "        self.first = first\n",
    "\n",
    "        if not self.first:\n",
    "            self.bn1 = nn.BatchNorm2d(num_features=nb_filts[0])\n",
    "        self.conv1 = nn.Conv2d(in_channels=nb_filts[0],\n",
    "                               out_channels=nb_filts[1],\n",
    "                               kernel_size=(2, 3),\n",
    "                               padding=(1, 1),\n",
    "                               stride=1)\n",
    "        self.selu = nn.SELU(inplace=True)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=nb_filts[1])\n",
    "        self.conv2 = nn.Conv2d(in_channels=nb_filts[1],\n",
    "                               out_channels=nb_filts[1],\n",
    "                               kernel_size=(2, 3),\n",
    "                               padding=(0, 1),\n",
    "                               stride=1)\n",
    "\n",
    "        if nb_filts[0] != nb_filts[1]:\n",
    "            self.downsample = True\n",
    "            self.conv_downsample = nn.Conv2d(in_channels=nb_filts[0],\n",
    "                                             out_channels=nb_filts[1],\n",
    "                                             padding=(0, 1),\n",
    "                                             kernel_size=(1, 3),\n",
    "                                             stride=1)\n",
    "\n",
    "        else:\n",
    "            self.downsample = False\n",
    "        self.mp = nn.MaxPool2d((1, 3))  # self.mp = nn.MaxPool2d((1,4))\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if not self.first:\n",
    "            out = self.bn1(x)\n",
    "            out = self.selu(out)\n",
    "        else:\n",
    "            out = x\n",
    "        out = self.conv1(x)\n",
    "\n",
    "        # print('out',out.shape)\n",
    "        out = self.bn2(out)\n",
    "        out = self.selu(out)\n",
    "        # print('out',out.shape)\n",
    "        out = self.conv2(out)\n",
    "        #print('conv2 out',out.shape)\n",
    "        if self.downsample:\n",
    "            identity = self.conv_downsample(identity)\n",
    "\n",
    "        out += identity\n",
    "        out = self.mp(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, d_args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_args = d_args\n",
    "        filts = d_args[\"filts\"]\n",
    "        gat_dims = d_args[\"gat_dims\"]\n",
    "        pool_ratios = d_args[\"pool_ratios\"]\n",
    "        temperatures = d_args[\"temperatures\"]\n",
    "\n",
    "        self.conv_time = CONV(out_channels=filts[0],\n",
    "                              kernel_size=d_args[\"first_conv\"],\n",
    "                              in_channels=1)\n",
    "        self.first_bn = nn.BatchNorm2d(num_features=1)\n",
    "\n",
    "        self.drop = nn.Dropout(0.5, inplace=True)\n",
    "        self.drop_way = nn.Dropout(0.2, inplace=True)\n",
    "        self.selu = nn.SELU(inplace=True)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[1], first=True)),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[2])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[3])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[4])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[4])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[4])))\n",
    "\n",
    "        self.pos_S = nn.Parameter(torch.randn(1, 23, filts[-1][-1]))\n",
    "        self.master1 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n",
    "        self.master2 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n",
    "\n",
    "        self.GAT_layer_S = GraphAttentionLayer(filts[-1][-1],\n",
    "                                               gat_dims[0],\n",
    "                                               temperature=temperatures[0])\n",
    "        self.GAT_layer_T = GraphAttentionLayer(filts[-1][-1],\n",
    "                                               gat_dims[0],\n",
    "                                               temperature=temperatures[1])\n",
    "\n",
    "        self.HtrgGAT_layer_ST11 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n",
    "        self.HtrgGAT_layer_ST12 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n",
    "\n",
    "        self.HtrgGAT_layer_ST21 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n",
    "\n",
    "        self.HtrgGAT_layer_ST22 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n",
    "\n",
    "        self.pool_S = GraphPool(pool_ratios[0], gat_dims[0], 0.3)\n",
    "        self.pool_T = GraphPool(pool_ratios[1], gat_dims[0], 0.3)\n",
    "        self.pool_hS1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "        self.pool_hT1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "\n",
    "        self.pool_hS2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "        self.pool_hT2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "\n",
    "        self.out_layer1 = nn.Linear(5 * gat_dims[1], 2)\n",
    "        self.out_layer2 = nn.Linear(5 * gat_dims[1], 6)\n",
    "\n",
    "\n",
    "    def forward(self, x, Freq_aug=False):\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_time(x, mask=Freq_aug)\n",
    "        x = x.unsqueeze(dim=1)\n",
    "        x = F.max_pool2d(torch.abs(x), (3, 3))\n",
    "        x = self.first_bn(x)\n",
    "        x = self.selu(x)\n",
    "\n",
    "        # get embeddings using encoder\n",
    "        # (#bs, #filt, #spec, #seq)\n",
    "        e = self.encoder(x)\n",
    "\n",
    "        # spectral GAT (GAT-S)\n",
    "        e_S, _ = torch.max(torch.abs(e), dim=3)  # max along time\n",
    "        e_S = e_S.transpose(1, 2) + self.pos_S\n",
    "\n",
    "        gat_S = self.GAT_layer_S(e_S)\n",
    "        out_S = self.pool_S(gat_S)  # (#bs, #node, #dim)\n",
    "\n",
    "        # temporal GAT (GAT-T)\n",
    "        e_T, _ = torch.max(torch.abs(e), dim=2)  # max along freq\n",
    "        e_T = e_T.transpose(1, 2)\n",
    "\n",
    "        gat_T = self.GAT_layer_T(e_T)\n",
    "        out_T = self.pool_T(gat_T)\n",
    "\n",
    "        # learnable master node\n",
    "        master1 = self.master1.expand(x.size(0), -1, -1)\n",
    "        master2 = self.master2.expand(x.size(0), -1, -1)\n",
    "\n",
    "        # inference 1\n",
    "        out_T1, out_S1, master1 = self.HtrgGAT_layer_ST11(\n",
    "            out_T, out_S, master=self.master1)\n",
    "\n",
    "        out_S1 = self.pool_hS1(out_S1)\n",
    "        out_T1 = self.pool_hT1(out_T1)\n",
    "\n",
    "        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST12(\n",
    "            out_T1, out_S1, master=master1)\n",
    "        out_T1 = out_T1 + out_T_aug\n",
    "        out_S1 = out_S1 + out_S_aug\n",
    "        master1 = master1 + master_aug\n",
    "\n",
    "        # inference 2\n",
    "        out_T2, out_S2, master2 = self.HtrgGAT_layer_ST21(\n",
    "            out_T, out_S, master=self.master2)\n",
    "        out_S2 = self.pool_hS2(out_S2)\n",
    "        out_T2 = self.pool_hT2(out_T2)\n",
    "\n",
    "        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST22(\n",
    "            out_T2, out_S2, master=master2)\n",
    "        out_T2 = out_T2 + out_T_aug\n",
    "        out_S2 = out_S2 + out_S_aug\n",
    "        master2 = master2 + master_aug\n",
    "\n",
    "        out_T1 = self.drop_way(out_T1)\n",
    "        out_T2 = self.drop_way(out_T2)\n",
    "        out_S1 = self.drop_way(out_S1)\n",
    "        out_S2 = self.drop_way(out_S2)\n",
    "        master1 = self.drop_way(master1)\n",
    "        master2 = self.drop_way(master2)\n",
    "\n",
    "        out_T = torch.max(out_T1, out_T2)\n",
    "        out_S = torch.max(out_S1, out_S2)\n",
    "        master = torch.max(master1, master2)\n",
    "\n",
    "        T_max, _ = torch.max(torch.abs(out_T), dim=1)\n",
    "        T_avg = torch.mean(out_T, dim=1)\n",
    "\n",
    "        S_max, _ = torch.max(torch.abs(out_S), dim=1)\n",
    "        S_avg = torch.mean(out_S, dim=1)\n",
    "\n",
    "        last_hidden = torch.cat(\n",
    "            [T_max, T_avg, S_max, S_avg, master.squeeze(1)], dim=1)\n",
    "\n",
    "        last_hidden = self.drop(last_hidden)\n",
    "        output1 = self.out_layer1(last_hidden)\n",
    "        output2 = self.out_layer2(last_hidden)\n",
    "\n",
    "        return output1, output2\n",
    "\n",
    "d_args = {\n",
    "        \"architecture\": \"AASIST\",\n",
    "        \"nb_samp\": 80000,\n",
    "        \"first_conv\": 128,\n",
    "        \"filts\": [70, [1, 32], [32, 32], [32, 64], [64, 64]],\n",
    "        \"gat_dims\": [64, 32],\n",
    "        \"pool_ratios\": [0.5, 0.7, 0.5, 0.5],\n",
    "        \"temperatures\": [2.0, 2.0, 100.0, 100.0]\n",
    "    }\n",
    "audio_model = Model(d_args=d_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NO PRETRAINING\n",
    "\n",
    "# audio_model = W2VAASIST()\n",
    "# # audio_model = torch.load(\"/home/hyj/ChanHyung/Audio/fake_voice_detection/anti-spoofing_feat_model.pt\")\n",
    "# audio_model(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score: 0.16765624564141035 auc: 0.875 brier: 0.12062500044703484 ece: 0.29999998211860657\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics.classification import BinaryAUROC, BinaryCalibrationError\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "class DaconMetrics(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(DaconMetrics, self).__init__()\n",
    "        self.auc_metric_class_0 = BinaryAUROC().to(device)\n",
    "        self.auc_metric_class_1 = BinaryAUROC().to(device)\n",
    "        self.brier_metric_class_0 = MeanSquaredError().to(device)\n",
    "        self.brier_metric_class_1 = MeanSquaredError().to(device)\n",
    "        self.ece_metric_class_0 = BinaryCalibrationError(n_bins=10, norm='l1').to(device)\n",
    "        self.ece_metric_class_1 = BinaryCalibrationError(n_bins=10, norm='l1').to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, pred, label):\n",
    "        # For class 0\n",
    "        self.auc_metric_class_0.update(pred[:, 0].detach(), label[:, 0].detach())\n",
    "        self.brier_metric_class_0.update(pred[:, 0].detach(), label[:, 0].detach())\n",
    "        self.ece_metric_class_0.update(pred[:, 0].detach(), label[:, 0].detach())\n",
    "\n",
    "        # For class 1\n",
    "        self.auc_metric_class_1.update(pred[:, 1].detach(), label[:, 1].detach())\n",
    "        self.brier_metric_class_1.update(pred[:, 1].detach(), label[:, 1].detach())\n",
    "        self.ece_metric_class_1.update(pred[:, 1].detach(), label[:, 1].detach())\n",
    "\n",
    "    def reset(self):\n",
    "        self.auc_metric_class_0.reset()\n",
    "        self.auc_metric_class_1.reset()\n",
    "        self.brier_metric_class_0.reset()\n",
    "        self.brier_metric_class_1.reset()\n",
    "        self.ece_metric_class_0.reset()\n",
    "        self.ece_metric_class_1.reset()\n",
    "\n",
    "    def compute(self):\n",
    "        with torch.no_grad():\n",
    "            auc_class_0 = self.auc_metric_class_0.compute().item()\n",
    "            auc_class_1 = self.auc_metric_class_1.compute().item()\n",
    "            mean_auc = (auc_class_0 + auc_class_1) / 2\n",
    "\n",
    "            mse_class_0 = self.brier_metric_class_0.compute().item()\n",
    "            mse_class_1 = self.brier_metric_class_1.compute().item()\n",
    "            mean_mse = (mse_class_0 + mse_class_1) / 2\n",
    "\n",
    "            ece_class_0 = self.ece_metric_class_0.compute().item()\n",
    "            ece_class_1 = self.ece_metric_class_1.compute().item()\n",
    "            mean_ece = (ece_class_0 + ece_class_1) / 2\n",
    "\n",
    "            final = 0.5 * (1 - mean_auc) + 0.25 * mean_mse + 0.25 * mean_ece\n",
    "\n",
    "        return final, mean_auc, mean_mse, mean_ece\n",
    "    \n",
    "\n",
    "dacon_metrics = DaconMetrics(torch.device('cpu'))\n",
    "\n",
    "# Example data\n",
    "pred = torch.tensor([[0.9, 0.1], [0.6, 0.4], [0.65, 0.35], [0.2, 0.8]])  \n",
    "label = torch.tensor([[1, 0], [1, 0], [1, 1], [0, 1]])\n",
    "\n",
    "# Update metrics\n",
    "dacon_metrics(pred, label)\n",
    "# Compute results\n",
    "final, auc, brier, ece = dacon_metrics.compute()\n",
    "print(f'Final Score: {final} auc: {auc} brier: {brier} ece: {ece}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader, criterion1, criterion2):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    valid_metrics = DaconMetrics(device=device)\n",
    "\n",
    "    # Initialize F1 score metric\n",
    "    valid_loader_tqdm = tqdm(valid_loader, desc=f\"Valid \")\n",
    "    with torch.no_grad():\n",
    "        for x, y, y_trans in valid_loader_tqdm:\n",
    "            x = x.to(device)\n",
    "            y_trans = y_trans.to(device)\n",
    "            y = y.long().to(device)\n",
    "\n",
    "            output = model(x)\n",
    "            loss1 = criterion1(output[0], y_trans)\n",
    "            loss2 = criterion2(output[1], y)\n",
    "            loss = loss1 * 0.65 + loss2 * 0.35\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            # Update metrics\n",
    "            probs = torch.sigmoid(output[0])\n",
    "\n",
    "            # Update metrics\n",
    "            valid_metrics(probs, y_trans)\n",
    "            dacon_score, auc, brier, ece = valid_metrics.compute()\n",
    "            \n",
    "            valid_loader_tqdm.set_postfix({'loss': np.mean(val_loss), 'dacon_score': dacon_score, 'auc': auc, 'brier': brier, 'ece': ece})\n",
    "            \n",
    "    avg_loss = np.mean(val_loss)\n",
    "    avg_dacon_score, avg_acu, avg_brier, avg_ece = valid_metrics.compute()\n",
    "\n",
    "    return avg_loss, avg_dacon_score, avg_acu, avg_brier, avg_ece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, optimizer, scheduler):\n",
    "    model.to(device)\n",
    "    criterion1 = nn.MultiLabelSoftMarginLoss().to(device)\n",
    "    criterion2 = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    best_model = None\n",
    "    best_dacon = 14651651513221231231\n",
    "    train_metrics = DaconMetrics(device=device)\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "        train_loss = []\n",
    "        model.train()\n",
    "\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch} Training\")\n",
    "        for i, (x, y, y_trans) in enumerate(train_loader_tqdm):\n",
    "            x = x.to(device)\n",
    "            y_trans = y_trans.to(device)\n",
    "            y = y.long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss1 = criterion1(output[0], y_trans)\n",
    "            loss2 = criterion2(output[1], y)\n",
    "            loss = loss1 * 0.65 + loss2 * 0.35\n",
    "            loss.backward()\n",
    "\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                probs = torch.sigmoid(output[0])\n",
    "                train_metrics(probs, y_trans)\n",
    "                dacon_score, auc, brier, ece = train_metrics.compute()\n",
    "\n",
    "        \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            train_loader_tqdm.set_postfix({\n",
    "                'loss': np.mean(train_loss),\n",
    "                'dacon_score': dacon_score,\n",
    "                'auc': auc,\n",
    "                'brier': brier,\n",
    "                'ece': ece\n",
    "            })\n",
    "\n",
    "            # if i % 3 == 0:\n",
    "            #     wandb.log({\n",
    "            #         'train_loss': np.mean(train_loss),\n",
    "            #         'train_dacon_score': dacon_score,\n",
    "            #         'train_auc': auc,\n",
    "            #         'train_brier': brier,\n",
    "            #         'train_ece': ece\n",
    "            #     })\n",
    "\n",
    "\n",
    "\n",
    "        dacon_score, auc, brier, ece = train_metrics.compute()\n",
    "\n",
    "        valid_loss, valid_dacon, valid_acu, valid_brier, valid_ece = validation(model, valid_loader, criterion1, criterion2)\n",
    "\n",
    "        \n",
    "        # wandb.log({\n",
    "        #         'valid_loss': valid_loss,\n",
    "        #         'valid_dacon_score': valid_dacon,\n",
    "        #         'valid_auc': valid_acu,\n",
    "        #         'valid_brier': valid_brier,\n",
    "        #         'valid_ece': valid_ece\n",
    "        #     })\n",
    "        print(f'Epoch {epoch} - step: [{i + 1}/{num_batches}] train_loss:[{np.mean(train_loss):.5f}] dacon_score:[{dacon_score:.5f}] auc:[{auc:.5f}] brier:[{brier:.5f}] ece:[{ece:.5f}]')\n",
    "        print(f'Validation - step: [{i + 1}/{num_batches}] valid_loss:[{valid_loss:.5f}] valid_dacon:[{valid_dacon:.5f}] valid_auc:[{valid_acu:.5f}] valid_brier:[{valid_brier:.5f}] valid_ece:[{valid_ece:.5f}]')\n",
    "        train_metrics.reset()\n",
    "\n",
    "\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_dacon)\n",
    "\n",
    "        if valid_dacon < best_dacon:\n",
    "            best_dacon = valid_dacon\n",
    "            best_model = model\n",
    "            print(f\"{best_dacon} saved.\")\n",
    "        torch.save(model.state_dict(), f\"./model_save_epoch_{epoch}_aasist_bandstop_addzero_new_dataset_2loss.pth\")\n",
    "\n",
    "    print(f'best_dacon:{best_dacon:.5f}')\n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(audio_model.parameters(), lr=CFG['LR'], weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/3149 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 3149/3149 [20:02<00:00,  2.62it/s, loss=0.586, dacon_score=0.0772, auc=0.912, brier=0.119, ece=0.0127]\n",
      "Valid : 100%|██████████| 200/200 [00:26<00:00,  7.51it/s, loss=0.842, dacon_score=0.142, auc=0.874, brier=0.176, ece=0.139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - step: [3149/3149] train_loss:[0.58636] dacon_score:[0.07716] auc:[0.91152] brier:[0.11899] ece:[0.01270]\n",
      "Validation - step: [3149/3149] valid_loss:[0.84231] valid_dacon:[0.14183] valid_auc:[0.87356] valid_brier:[0.17570] valid_ece:[0.13875]\n",
      "0.14183190930634737 saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 3149/3149 [19:56<00:00,  2.63it/s, loss=0.37, dacon_score=0.0372, auc=0.965, brier=0.0727, ece=0.00603] \n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.70it/s, loss=1.28, dacon_score=0.205, auc=0.848, brier=0.254, ece=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - step: [3149/3149] train_loss:[0.36959] dacon_score:[0.03723] auc:[0.96488] brier:[0.07266] ece:[0.00603]\n",
      "Validation - step: [3149/3149] valid_loss:[1.27747] valid_dacon:[0.20478] valid_auc:[0.84776] valid_brier:[0.25370] valid_ece:[0.26093]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 3149/3149 [19:54<00:00,  2.64it/s, loss=0.31, dacon_score=0.0291, auc=0.975, brier=0.0606, ece=0.00518] \n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.88it/s, loss=0.871, dacon_score=0.137, auc=0.899, brier=0.177, ece=0.168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - step: [3149/3149] train_loss:[0.30962] dacon_score:[0.02906] auc:[0.97476] brier:[0.06059] ece:[0.00518]\n",
      "Validation - step: [3149/3149] valid_loss:[0.87052] valid_dacon:[0.13652] valid_auc:[0.89934] valid_brier:[0.17722] valid_ece:[0.16753]\n",
      "0.13651809096336365 saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 3149/3149 [19:55<00:00,  2.63it/s, loss=0.276, dacon_score=0.0245, auc=0.98, brier=0.0532, ece=0.00435] \n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.87it/s, loss=0.794, dacon_score=0.108, auc=0.904, brier=0.144, ece=0.0957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - step: [3149/3149] train_loss:[0.27603] dacon_score:[0.02447] auc:[0.97984] brier:[0.05321] ece:[0.00435]\n",
      "Validation - step: [3149/3149] valid_loss:[0.79398] valid_dacon:[0.10818] valid_auc:[0.90361] valid_brier:[0.14423] valid_ece:[0.09571]\n",
      "0.10818029288202524 saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 3149/3149 [19:52<00:00,  2.64it/s, loss=0.255, dacon_score=0.0217, auc=0.983, brier=0.0485, ece=0.00412]\n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.89it/s, loss=0.713, dacon_score=0.0994, auc=0.899, brier=0.133, ece=0.0623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - step: [3149/3149] train_loss:[0.25465] dacon_score:[0.02174] auc:[0.98281] brier:[0.04845] ece:[0.00412]\n",
      "Validation - step: [3149/3149] valid_loss:[0.71284] valid_dacon:[0.09939] valid_auc:[0.89897] valid_brier:[0.13317] valid_ece:[0.06232]\n",
      "0.09938732162117958 saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|██████████| 3149/3149 [19:54<00:00,  2.64it/s, loss=0.239, dacon_score=0.02, auc=0.985, brier=0.0452, ece=0.00448]  \n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.91it/s, loss=0.7, dacon_score=0.105, auc=0.897, brier=0.138, ece=0.0782]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - step: [3149/3149] train_loss:[0.23885] dacon_score:[0.02001] auc:[0.98479] brier:[0.04515] ece:[0.00448]\n",
      "Validation - step: [3149/3149] valid_loss:[0.70007] valid_dacon:[0.10544] valid_auc:[0.89741] valid_brier:[0.13837] valid_ece:[0.07822]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training: 100%|██████████| 3149/3149 [19:49<00:00,  2.65it/s, loss=0.224, dacon_score=0.0182, auc=0.986, brier=0.0422, ece=0.00362]\n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.87it/s, loss=0.856, dacon_score=0.101, auc=0.92, brier=0.136, ece=0.108]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - step: [3149/3149] train_loss:[0.22439] dacon_score:[0.01822] auc:[0.98647] brier:[0.04220] ece:[0.00362]\n",
      "Validation - step: [3149/3149] valid_loss:[0.85636] valid_dacon:[0.10078] valid_auc:[0.92013] valid_brier:[0.13552] valid_ece:[0.10788]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training: 100%|██████████| 3149/3149 [19:46<00:00,  2.65it/s, loss=0.212, dacon_score=0.0166, auc=0.988, brier=0.0399, ece=0.00245]\n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.90it/s, loss=0.831, dacon_score=0.118, auc=0.909, brier=0.155, ece=0.133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - step: [3149/3149] train_loss:[0.21229] dacon_score:[0.01664] auc:[0.98788] brier:[0.03988] ece:[0.00245]\n",
      "Validation - step: [3149/3149] valid_loss:[0.83102] valid_dacon:[0.11768] valid_auc:[0.90865] valid_brier:[0.15488] valid_ece:[0.13316]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training: 100%|██████████| 3149/3149 [19:46<00:00,  2.65it/s, loss=0.189, dacon_score=0.0145, auc=0.99, brier=0.0348, ece=0.00379]\n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.89it/s, loss=0.74, dacon_score=0.0995, auc=0.906, brier=0.135, ece=0.0759] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - step: [3149/3149] train_loss:[0.18877] dacon_score:[0.01445] auc:[0.99037] brier:[0.03478] ece:[0.00379]\n",
      "Validation - step: [3149/3149] valid_loss:[0.73991] valid_dacon:[0.09954] valid_auc:[0.90629] valid_brier:[0.13480] valid_ece:[0.07595]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Training: 100%|██████████| 3149/3149 [19:46<00:00,  2.65it/s, loss=0.185, dacon_score=0.0139, auc=0.991, brier=0.034, ece=0.00303] \n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.94it/s, loss=0.728, dacon_score=0.0987, auc=0.923, brier=0.135, ece=0.106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - step: [3149/3149] train_loss:[0.18468] dacon_score:[0.01386] auc:[0.99079] brier:[0.03401] ece:[0.00303]\n",
      "Validation - step: [3149/3149] valid_loss:[0.72794] valid_dacon:[0.09870] valid_auc:[0.92312] valid_brier:[0.13483] valid_ece:[0.10620]\n",
      "0.09869708865880966 saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Training: 100%|██████████| 3149/3149 [19:43<00:00,  2.66it/s, loss=0.179, dacon_score=0.0134, auc=0.991, brier=0.0329, ece=0.00337]\n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.95it/s, loss=0.727, dacon_score=0.0964, auc=0.915, brier=0.132, ece=0.0836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - step: [3149/3149] train_loss:[0.17945] dacon_score:[0.01342] auc:[0.99128] brier:[0.03289] ece:[0.00337]\n",
      "Validation - step: [3149/3149] valid_loss:[0.72713] valid_dacon:[0.09644] valid_auc:[0.91480] valid_brier:[0.13177] valid_ece:[0.08360]\n",
      "0.0964397843927145 saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Training: 100%|██████████| 3149/3149 [19:45<00:00,  2.66it/s, loss=0.173, dacon_score=0.0126, auc=0.992, brier=0.0316, ece=0.00269]\n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.95it/s, loss=0.92, dacon_score=0.117, auc=0.923, brier=0.158, ece=0.158] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - step: [3149/3149] train_loss:[0.17271] dacon_score:[0.01263] auc:[0.99188] brier:[0.03157] ece:[0.00269]\n",
      "Validation - step: [3149/3149] valid_loss:[0.91976] valid_dacon:[0.11719] valid_auc:[0.92345] valid_brier:[0.15796] valid_ece:[0.15772]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Training: 100%|██████████| 3149/3149 [19:43<00:00,  2.66it/s, loss=0.168, dacon_score=0.0121, auc=0.992, brier=0.0307, ece=0.00247]\n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.95it/s, loss=0.742, dacon_score=0.0992, auc=0.922, brier=0.137, ece=0.104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - step: [3149/3149] train_loss:[0.16792] dacon_score:[0.01210] auc:[0.99238] brier:[0.03068] ece:[0.00247]\n",
      "Validation - step: [3149/3149] valid_loss:[0.74194] valid_dacon:[0.09921] valid_auc:[0.92228] valid_brier:[0.13716] valid_ece:[0.10425]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Training: 100%|██████████| 3149/3149 [19:45<00:00,  2.66it/s, loss=0.166, dacon_score=0.0119, auc=0.993, brier=0.0302, ece=0.00255]\n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.89it/s, loss=0.876, dacon_score=0.107, auc=0.931, brier=0.144, ece=0.144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - step: [3149/3149] train_loss:[0.16566] dacon_score:[0.01190] auc:[0.99256] brier:[0.03015] ece:[0.00255]\n",
      "Validation - step: [3149/3149] valid_loss:[0.87607] valid_dacon:[0.10671] valid_auc:[0.93066] valid_brier:[0.14430] valid_ece:[0.14385]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Training: 100%|██████████| 3149/3149 [19:45<00:00,  2.66it/s, loss=0.152, dacon_score=0.0108, auc=0.994, brier=0.0275, ece=0.00296]\n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.95it/s, loss=0.75, dacon_score=0.098, auc=0.916, brier=0.133, ece=0.0903]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - step: [3149/3149] train_loss:[0.15229] dacon_score:[0.01079] auc:[0.99364] brier:[0.02749] ece:[0.00296]\n",
      "Validation - step: [3149/3149] valid_loss:[0.75030] valid_dacon:[0.09799] valid_auc:[0.91583] valid_brier:[0.13335] valid_ece:[0.09026]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Training: 100%|██████████| 3149/3149 [19:43<00:00,  2.66it/s, loss=0.15, dacon_score=0.0106, auc=0.994, brier=0.0268, ece=0.00326] \n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.93it/s, loss=0.951, dacon_score=0.124, auc=0.913, brier=0.169, ece=0.154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - step: [3149/3149] train_loss:[0.14960] dacon_score:[0.01058] auc:[0.99386] brier:[0.02679] ece:[0.00326]\n",
      "Validation - step: [3149/3149] valid_loss:[0.95093] valid_dacon:[0.12435] valid_auc:[0.91283] valid_brier:[0.16880] valid_ece:[0.15429]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Training: 100%|██████████| 3149/3149 [19:46<00:00,  2.65it/s, loss=0.147, dacon_score=0.0101, auc=0.994, brier=0.0261, ece=0.00264] \n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.92it/s, loss=1, dacon_score=0.127, auc=0.904, brier=0.171, ece=0.147]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - step: [3149/3149] train_loss:[0.14658] dacon_score:[0.01011] auc:[0.99418] brier:[0.02613] ece:[0.00264]\n",
      "Validation - step: [3149/3149] valid_loss:[1.00203] valid_dacon:[0.12744] valid_auc:[0.90378] valid_brier:[0.17051] valid_ece:[0.14680]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Training: 100%|██████████| 3149/3149 [19:44<00:00,  2.66it/s, loss=0.138, dacon_score=0.00928, auc=0.995, brier=0.0243, ece=0.00272]\n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.96it/s, loss=0.951, dacon_score=0.117, auc=0.921, brier=0.156, ece=0.152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - step: [3149/3149] train_loss:[0.13759] dacon_score:[0.00928] auc:[0.99497] brier:[0.02434] ece:[0.00272]\n",
      "Validation - step: [3149/3149] valid_loss:[0.95143] valid_dacon:[0.11667] valid_auc:[0.92073] valid_brier:[0.15590] valid_ece:[0.15223]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 Training: 100%|██████████| 3149/3149 [19:44<00:00,  2.66it/s, loss=0.137, dacon_score=0.00926, auc=0.995, brier=0.0242, ece=0.0026] \n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.89it/s, loss=0.983, dacon_score=0.12, auc=0.917, brier=0.16, ece=0.154]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - step: [3149/3149] train_loss:[0.13679] dacon_score:[0.00926] auc:[0.99489] brier:[0.02421] ece:[0.00260]\n",
      "Validation - step: [3149/3149] valid_loss:[0.98272] valid_dacon:[0.11969] valid_auc:[0.91734] valid_brier:[0.15983] valid_ece:[0.15361]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Training: 100%|██████████| 3149/3149 [19:44<00:00,  2.66it/s, loss=0.136, dacon_score=0.00911, auc=0.995, brier=0.0242, ece=0.00219]\n",
      "Valid : 100%|██████████| 200/200 [00:25<00:00,  7.95it/s, loss=0.894, dacon_score=0.109, auc=0.924, brier=0.149, ece=0.134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - step: [3149/3149] train_loss:[0.13607] dacon_score:[0.00911] auc:[0.99496] brier:[0.02419] ece:[0.00219]\n",
      "Validation - step: [3149/3149] valid_loss:[0.89409] valid_dacon:[0.10876] valid_auc:[0.92406] valid_brier:[0.14872] valid_ece:[0.13442]\n",
      "best_dacon:0.09644\n"
     ]
    }
   ],
   "source": [
    "infer_model = train(audio_model, train_loader, valid_loader, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(os.getcwd(), 'dataset', 'dacon_dataset')\n",
    "test_x = [f\"{base_path}/test_bandstop_denoising/enhanced_{i}.wav\" for i in test_df['id'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = process_batch(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataSet(test_x, None)\n",
    "test_loader = create_data_loader(test_dataset, CFG['BATCH_SIZE'], False, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model.load_state_dict(torch.load('./model_save_epoch_4_aasist_bandstop_addzero_new_dataset_2loss.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiLabelTemperatureScaling(nn.Module):\n",
    "    def __init__(self, temperature_fake_divide=1.5, temperature_real_divide=1.5):\n",
    "        super(MultiLabelTemperatureScaling, self).__init__()\n",
    "        # self.temperature_multiply = nn.Parameter(torch.ones(1) * temperature_multiply)\n",
    "        # self.temperature_divide = nn.Parameter(torch.ones(1) * temperature_divide)\n",
    "        self.temperature_fake_divide = nn.Parameter(torch.ones(1) * temperature_fake_divide)\n",
    "        self.temperature_real_divide = nn.Parameter(torch.ones(1) * temperature_real_divide)\n",
    "\n",
    "    def forward(self, logits):\n",
    "        # Apply temperature scaling to logits\n",
    "        scaled_logits = logits.clone()\n",
    "        scaled_logits[:, 0] = logits[:, 0] / self.temperature_fake_divide\n",
    "        scaled_logits[:, 1] = logits[:, 1] / self.temperature_real_divide\n",
    "        # Convert scaled logits to probabilities\n",
    "        probabilities = torch.sigmoid(scaled_logits)\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference : 100%|██████████| 2084/2084 [03:44<00:00,  9.27it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "audio_model.to(device)\n",
    "audio_model.eval()\n",
    "preds = []\n",
    "test_labels = []\n",
    "temperature_scaling = MultiLabelTemperatureScaling(temperature_fake_divide=3.5, temperature_real_divide=3.5).to(device)\n",
    "\n",
    "# Initialize F1 score metric\n",
    "test_loader_tqdm = tqdm(test_loader, desc=f\"inference \")\n",
    "with torch.no_grad():\n",
    "    for x in test_loader_tqdm:\n",
    "        x = x.to(device)\n",
    "        output = audio_model(x)\n",
    "\n",
    "        probs = temperature_scaling(output[0])\n",
    "        preds.append(probs.cpu().detach())\n",
    "\n",
    "        probabilities = F.softmax(output[1], dim=1)\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "        for i in range(predictions.size(0)):\n",
    "            test_labels.append(predictions[i].item())  \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 25785, 4: 7903, 5: 6373, 0: 4499, 3: 3448, 2: 1992})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(test_labels)\n",
    "\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.vstack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:01<00:00, 34393.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def check_boundaries_and_update_pred(index, file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if not data.get('boundaries'): \n",
    "                preds[index] = [0, 0]\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist\")\n",
    "\n",
    "for i in tqdm(range(50000)):\n",
    "    name = f\"enhanced_TEST_{i:05d}_boundaries.json\"\n",
    "    path = f\"./vad_bandstop_boundaries_test/{name}\"\n",
    "    check_boundaries_and_update_pred(i, path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./dataset/dacon_dataset/sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "submit.to_csv('./sw24_4label_epoch6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isSubmitted': True, 'detail': 'Success'}\n"
     ]
    }
   ],
   "source": [
    "from dacon_submit_api import dacon_submit_api \n",
    "\n",
    "result = dacon_submit_api.post_submission_file(\n",
    "'./sw24_4label_epoch6.csv', \n",
    "'', \n",
    "'236253', \n",
    "'어떻게 너의 목소리를 잊겠어', \n",
    "'' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cHb_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
