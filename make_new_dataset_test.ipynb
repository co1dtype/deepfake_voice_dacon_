{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1hi1dibkHyFbaxAteLlZJw6r3g9ddd4Lf\n",
      "From (redirected): https://drive.google.com/uc?id=1hi1dibkHyFbaxAteLlZJw6r3g9ddd4Lf&confirm=t&uuid=19f496fc-41e8-4478-b6a1-2508f41c8bed\n",
      "To: /home/hyj/ChanHyung/Audio/DACON_fake_voice_detection/dataset/dacon_dataset.zip\n",
      "100%|██████████| 3.31G/3.31G [02:07<00:00, 26.0MB/s]\n",
      "Extracting files: 100%|██████████| 106708/106708 [00:24<00:00, 4304.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and extracted to ./dataset/dacon_dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_id = \"1hi1dibkHyFbaxAteLlZJw6r3g9ddd4Lf\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "download_path = \"./dataset/dacon_dataset.zip\"\n",
    "extract_path = \"./dataset/dacon_dataset\"\n",
    "\n",
    "if not os.path.exists(extract_path):\n",
    "    os.makedirs(extract_path)\n",
    "\n",
    "gdown.download(url, download_path, quiet=False)\n",
    "\n",
    "with ZipFile(download_path, 'r') as zip_ref:\n",
    "    for file in tqdm(zip_ref.namelist(), desc='Extracting files'):\n",
    "        zip_ref.extract(file, extract_path)\n",
    "\n",
    "os.remove(download_path)\n",
    "\n",
    "print(f\"File downloaded and extracted to {extract_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyj/anaconda3/envs/DACON_DEEPFAKE/lib/python3.11/site-packages/df/io.py:9: UserWarning: `torchaudio.backend.common.AudioMetaData` has been moved to `torchaudio.AudioMetaData`. Please update the import path.\n",
      "  from torchaudio.backend.common import AudioMetaData\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "\n",
    "from df import config\n",
    "from df.enhance import enhance, init_df, load_audio, save_audio\n",
    "from df.io import resample\n",
    "\n",
    "from torchaudio import AudioMetaData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hyj/ChanHyung/Audio/DACON_fake_voice_detection'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_path = os.getcwd()\n",
    "train_df = pd.read_csv(f'{base_path}/dataset/dacon_dataset/train.csv')\n",
    "test_df = pd.read_csv(f'{base_path}/dataset/dacon_dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 02:41:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on torch 2.3.0\u001b[0m\n",
      "\u001b[32m2024-07-23 02:41:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on host cvlab\u001b[0m\n",
      "\u001b[32m2024-07-23 02:41:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mLoading model settings of DeepFilterNet2\u001b[0m\n",
      "\u001b[32m2024-07-23 02:41:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mInitializing model `deepfilternet2`\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any of the parent directories): .git\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 02:41:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mFound checkpoint /home/hyj/ChanHyung/Audio/DACON_fake_voice_detection/DeepFilterNet2/checkpoints/model_96.ckpt.best with epoch 96\u001b[0m\n",
      "\u001b[32m2024-07-23 02:41:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on device cuda:0\u001b[0m\n",
      "\u001b[32m2024-07-23 02:41:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mModel loaded\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5821d791c224e9aaf43a70d29ac3820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-23 02:41:10\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[33m\u001b[1mAudio sampling rate does not match model sampling rate (32000, 48000). Resampling...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import shutil\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "dir_name = 'test_bandstop_denoising'\n",
    "os.makedirs(f\"./dataset/dacon_dataset/{dir_name}\", exist_ok=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, df, _ = init_df(os.path.join(base_path, 'DeepFilterNet2'), config_allow_defaults=True)\n",
    "model = model.to(device=device).eval()\n",
    "test_x = os.listdir(os.path.join(base_path, 'dataset', 'dacon_dataset', 'test'))\n",
    "\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    id = test_x[i].split('/')[-1].split('.')[0]\n",
    "    try: \n",
    "        path = os.path.join(base_path, 'dataset', 'dacon_dataset', 'test', test_x[i])\n",
    "        sr = config(\"sr\", 32000, int, section=\"df\")\n",
    "        sample, meta = load_audio(path, sr)\n",
    "        enhanced = enhance(model, df, sample)\n",
    "\n",
    "        lim = torch.linspace(0.0, 1.0, int(sr * 0.15)).unsqueeze(0)\n",
    "        lim = torch.cat((lim, torch.ones(1, enhanced.shape[1] - lim.shape[1])), dim=1)\n",
    "        enhanced = enhanced * lim\n",
    "        if meta.sample_rate != sr:\n",
    "            enhanced = resample(enhanced, sr, meta.sample_rate)\n",
    "            sample = resample(sample, sr, meta.sample_rate)\n",
    "            sr = meta.sample_rate\n",
    "\n",
    "        noisy_wav = os.path.join(base_path, 'noisy_wav1557.wav')\n",
    "        save_audio(noisy_wav, sample, sr)\n",
    "        enhanced_wav = os.path.join(base_path, 'enhanced_wav1557.wav') \n",
    "        save_audio(enhanced_wav, enhanced, sr)\n",
    "        noisy_wav = os.path.join(base_path, 'dataset', 'dacon_dataset', dir_name, f'noisy_{id}.wav')\n",
    "        shutil.move(os.path.join(base_path, 'noisy_wav1557.wav'), noisy_wav)\n",
    "        enhanced_wav = os.path.join(base_path, 'dataset', 'dacon_dataset', dir_name, f'enhanced_{id}.wav')\n",
    "        shutil.move(os.path.join(base_path, 'enhanced_wav1557.wav') ,  enhanced_wav)\n",
    "    except:\n",
    "        print(f\"너무 짧아서 안됨: {id}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_list = [os.path.join(base_path, 'dataset', 'dacon_dataset', 'test_bandstop_denoising', i) \\\n",
    "                   for i in os.listdir(os.path.join(base_path, 'dataset', 'dacon_dataset', 'test_bandstop_denoising')) if \"noisy\" not in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [5:41:57<00:00,  2.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All boundaries and audio lengths have been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from speechbrain.inference.VAD import VAD\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# VAD 모델 초기화\n",
    "vad_model = VAD.from_hparams(source=\"speechbrain/vad-crdnn-libriparty\", savedir=\"pretrained_models/vad-crdnn-libriparty\")\n",
    "\n",
    "# 오디오 파일 경로 리스트\n",
    "audio_file_list = [os.path.join(base_path, 'dataset', 'dacon_dataset', 'test_bandstop_denoising', i) \\\n",
    "                   for i in os.listdir(os.path.join(base_path, 'dataset', 'dacon_dataset', 'test_bandstop_denoising')) if \"noisy\" not in i]\n",
    "\n",
    "\n",
    "# 타겟 샘플링 레이트\n",
    "target_sample_rate = 16000\n",
    "\n",
    "# 음성 구간 정보를 저장할 디렉토리\n",
    "output_dir = os.path.join(base_path, 'vad_bandstop_boundaries_test')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 오디오 파일 처리\n",
    "for input_file in tqdm(audio_file_list):\n",
    "    # Step 1: 오디오 파일 로드 및 샘플링 레이트 변환\n",
    "    signal, original_sample_rate = torchaudio.load(input_file)\n",
    "    transform = torchaudio.transforms.Resample(orig_freq=original_sample_rate, new_freq=target_sample_rate)\n",
    "    resampled_signal = transform(signal)\n",
    "    \n",
    "    # 오디오 길이 계산 (초 단위)\n",
    "    audio_length = resampled_signal.size(1) / target_sample_rate\n",
    "    \n",
    "    # 임시 파일로 저장\n",
    "    temp_output_file = os.path.join(base_path, 'temp_resampled_audio_file.wav')\n",
    "    torchaudio.save(temp_output_file, resampled_signal, target_sample_rate)\n",
    "    \n",
    "    # Step 2: 변환된 파일로 VAD 수행\n",
    "    boundaries = vad_model.get_speech_segments(temp_output_file)\n",
    "    \n",
    "    # Step 3: 각 음성 구간의 시작 시간과 종료 시간을 기록\n",
    "    base_filename = os.path.splitext(os.path.basename(input_file))[0]\n",
    "    boundaries_list = []\n",
    "    for boundary in boundaries:\n",
    "        start, end = boundary[0].item(), boundary[1].item()\n",
    "        boundaries_list.append({\n",
    "            \"start\": start,\n",
    "            \"end\": end\n",
    "        })\n",
    "    \n",
    "    # JSON 파일로 저장\n",
    "    output_data = {\n",
    "        \"audio_length\": audio_length,\n",
    "        \"boundaries\": boundaries_list\n",
    "    }\n",
    "    boundaries_filename = f\"{base_filename}_boundaries.json\"\n",
    "    boundaries_filepath = os.path.join(output_dir, boundaries_filename)\n",
    "    with open(boundaries_filepath, 'w') as f:\n",
    "        json.dump(output_data, f)\n",
    "    \n",
    "    # print(f\"Saved boundaries and audio length for {input_file} to {boundaries_filepath}\")\n",
    "    \n",
    "    # 임시 파일 삭제\n",
    "    os.remove(temp_output_file)\n",
    "\n",
    "print(\"All boundaries and audio lengths have been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cHb_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
